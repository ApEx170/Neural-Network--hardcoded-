{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__ (self, inNodes, outNodes):\n",
    "        #initialize the current layer with number of inputs incomming and expected output of the layers\n",
    "        # Numer of input and output nodes\n",
    "\n",
    "        self.inNodes = inNodes + 1    #Bias also counted as a neuron \n",
    "        self.outNodes = outNodes\n",
    "        self.activation = np.full(outNodes, 0)\n",
    "        \n",
    "\t\t# Random initilization of Matrix of weights init + bias weight\n",
    "        self.weights = np.round(np.random.rand(self.inNodes, self.outNodes) - 0.5, 2)\n",
    "\n",
    "    def desc_layer(self):\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(f\"IN NODE + bias: {self.inNodes-1}+1\\nOUT NODES: {self.outNodes}\")\n",
    "        print(f\"weight: {self.weights}\")\n",
    "        print(f\"[0]-sigmoid\\n[1]-realu\\n[3]-tanh\\nActivation_stack_neuron: {self.activation}\")\n",
    "        print(\"--------------------------------------------------\\n\")\n",
    "    \n",
    "    \n",
    "    def activation_stack(self,activation_stack):\n",
    "        # defines the stack for identifing which activation func is applied on which neuron respectively \n",
    "        self.activation = activation_stack\n",
    "\n",
    "    def activationFunc(self,output):\n",
    "        # un-optimized yet, need to be improved \n",
    "        # problems with np.matrix leading to typecasting with np.array for element itterations \n",
    "        \n",
    "        output = np.array(output)\n",
    "        opertaion = [\n",
    "          lambda x: 1 / (1 + np.exp(-x)),\n",
    "            lambda x: np.maximum(0, x),\n",
    "            lambda x: np.tanh(x)\n",
    "            ] \n",
    "        for index in range(len(output[0])):\n",
    "            output[0][index] = opertaion[self.activation[index]](output[0][index])  \n",
    "        return np.matrix(output) \n",
    "   \n",
    "    def fwd(self, input): #foward pass\n",
    "        # Add the bias value to the input\n",
    "        self.input = np.concatenate((input, [[1]]), axis=1)\n",
    "        # Sum the inputs and normalize for each output; dot product of pervious inputs and outputs\n",
    "        sum = np.dot(self.input, self.weights)\n",
    "        self.output = self.activationFunc(sum)\n",
    "        print (self.output)\n",
    "        return self.output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "\n",
    "\tdef __init__ (self, features, hiddenNeurons, classes, hiddenLayers = 1):\n",
    "\n",
    "\t\t# Create the first layer(DEFAULT)\n",
    "\t\tself.layerStack = np.array([Layer(features, hiddenNeurons)])\n",
    "\n",
    "\t\t# Create the hidden layers, WILL CREATE MULTIPLE HIDDEN LAYER DEPENDING UPON THE INPUT PARAM\n",
    "\t\tfor x in range(hiddenLayers - 1):\n",
    "\t\t\tself.layerStack = np.append(self.layerStack, [Layer(hiddenNeurons, hiddenNeurons)])\n",
    "\n",
    "\t\t# Create the output layer(DEFAULT)\n",
    "\t\tself.layerStack = np.append(self.layerStack, [Layer(hiddenNeurons, classes)])\n",
    "\n",
    "\tdef def_weight(self,matrix,hiddenlayer=1):\n",
    "\t\tself.layerStack[hiddenlayer].weights = matrix\n",
    "\n",
    "\tdef desc_network(self):\n",
    "\t\tfor x in self.layerStack:\n",
    "\t\t\tx.desc_layer()\n",
    "\n",
    "\tdef def_layer_activation(self,stack,layer):\n",
    "\t\tself.layerStack[layer].activation_stack(stack)\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef eval(self, input):\n",
    "        # Forward the signal through the layers\n",
    "\t\tlastInput = input\n",
    "\t\tfor l in self.layerStack:\n",
    "\t\t\tlastInput = l.fwd(lastInput)\n",
    "\t\treturn lastInput\n",
    "\t\n",
    "\tdef train(self, input, target, iterations = 10000):\n",
    "\t\tfor j in range(input.shape[0]):\n",
    "\t\t\t\t # For each input vector in the data get the output\n",
    "\t\t\t\tinputVector = input[j]\n",
    "\t\t\t\tout = self.eval(inputVector)\n",
    "\t\t\t\tprint(f\"output for input vector {j}: {out}\\n\\n\")\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.matrix([[0, 0],\n",
    "               [1, 0],\n",
    "               [0, 1],\n",
    "               [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "IN NODE + bias: 2+1\n",
      "OUT NODES: 3\n",
      "weight: [[-0.39 -0.42 -0.08]\n",
      " [-0.18  0.41  0.06]\n",
      " [-0.12  0.26 -0.03]]\n",
      "[0]-sigmoid\n",
      "[1]-realu\n",
      "[3]-tanh\n",
      "Activation_stack_neuron: [0, 0, 1]\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "IN NODE + bias: 3+1\n",
      "OUT NODES: 2\n",
      "weight: [[ 0.07 -0.02]\n",
      " [ 0.09 -0.01]\n",
      " [ 0.43 -0.23]\n",
      " [ 0.24  0.05]]\n",
      "[0]-sigmoid\n",
      "[1]-realu\n",
      "[3]-tanh\n",
      "Activation_stack_neuron: [0 0]\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = NeuralNetwork(2, 3, 2)\n",
    "network.def_layer_activation([0,0,1],0)\n",
    "network.desc_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47003595 0.56463629 0.        ]]\n",
      "[[0.58023052 0.50873734]]\n",
      "output for input vector 0: [[0.58023052 0.50873734]]\n",
      "\n",
      "\n",
      "[[0.37519353 0.46008512 0.        ]]\n",
      "[[0.57631673 0.50947269]]\n",
      "output for input vector 1: [[0.57631673 0.50947269]]\n",
      "\n",
      "\n",
      "[[0.42555748 0.66150316 0.03      ]]\n",
      "[[0.58473073 0.506993  ]]\n",
      "output for input vector 2: [[0.58473073 0.506993  ]]\n",
      "\n",
      "\n",
      "[[0.33403307 0.5621765  0.        ]]\n",
      "[[0.577856   0.50942328]]\n",
      "output for input vector 3: [[0.577856   0.50942328]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network.train(input,1)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

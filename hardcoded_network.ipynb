{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__ (self, prevNodes, currNodes):\n",
    "        #initialize the current layer with number of inputs incomming and expected output of the layers\n",
    "        # Numer of input and output nodes\n",
    "        self.prevNodes = prevNodes + 1    #Bias also counted as a neuron \n",
    "        self.currNodes = currNodes\n",
    "        result_array = np.full(currNodes, 0)\n",
    "\t\t# Random initilization of Matrix of weights init + bias weight\n",
    "        self.weights = np.round(np.random.rand(self.prevNodes, self.currNodes) - 0.5, 2)\n",
    "\n",
    "    def activation_stack(self,activation_stack):\n",
    "        # defines the stack for identifing which activation func is applied on which neuron respectively \n",
    "        self.activation = activation_stack\n",
    "\n",
    "    def activationFunc(self,input):\n",
    "        \n",
    "        opertaion = [\n",
    "           1 / (1 + np.exp(-x)),\n",
    "            np.maximum(0, x),\n",
    "            np.tanh(x)\n",
    "            ]\n",
    "        \n",
    "        for x in range(len(input)):\n",
    "            input[x] = opertaion[self.activation[x]](input[x])\n",
    "        return input\n",
    "   \n",
    "    def fwd(self, input): #foward pass\n",
    "        # Add the bias value to the input\n",
    "        self.input = np.concatenate((input, [[1]]), axis=1)\n",
    "        # Sum the inputs and normalize for each output; dot product of pervious inputs and outputs\n",
    "        sum = np.dot(self.input, self.weights)\n",
    "        self.output = activationFunc(sum)\n",
    "        print (self.output)\n",
    "        return self.output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 18 (2427288642.py, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 21\u001b[1;36m\u001b[0m\n\u001b[1;33m    def eval(self, input):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 18\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(object):\n",
    "\n",
    "\tdef __init__ (self, features, hiddenNeurons, classes, hiddenLayers = 1):\n",
    "\n",
    "\t\t# Create the first layer(DEFAULT)\n",
    "\t\tself.layerStack = np.array([Layer(features, hiddenNeurons)])\n",
    "\n",
    "\t\t# Create the hidden layers, WILL CREATE MULTIPLE HIDDEN LAYER DEPENDING UPON THE INPUT PARAM\n",
    "\t\tfor x in range(hiddenLayers - 1):\n",
    "\t\t\tself.layerStack = np.append(self.layerStack, [Layer(hiddenNeurons, hiddenNeurons)])\n",
    "\n",
    "\t\t# Create the output layer(DEFAULT)\n",
    "\t\tself.layerStack = np.append(self.layerStack, [Layer(hiddenNeurons, classes)])\n",
    "\n",
    "\tdef def_weight(self,matrix,hiddenlayer=1):\n",
    "\t\tself.layerStack[hiddenlayer].weights = matrix\n",
    "\n",
    "\tdef def_activation(self,hiddenlayer):\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef eval(self, input):\n",
    "        # Forward the signal through the layers\n",
    "\t\tlastInput = input\n",
    "\t\tfor l in self.layerStack:\n",
    "\t\t\tlastInput = l.fwd(lastInput)\n",
    "\n",
    "\t\treturn lastInput\n",
    "\t\n",
    "\tdef train(self, input, target, iterations = 10000):\n",
    "\t\tfor j in range(input.shape[0]):\n",
    "\t\t\t\t # For each input vector in the data get the output\n",
    "\t\t\t\tinputVector = input[j]\n",
    "\t\t\t\tout = self.eval(inputVector)\n",
    "\t\t\t\tprint(f\"output for input vector {j}: {out} \")\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.matrix([[0, 0],\n",
    "               [1, 0],\n",
    "               [0, 1],\n",
    "               [1, 1]])\n",
    "weight =  np.matrix([[0, 0],\n",
    "               [1, 0],\n",
    "               [0, 1],\n",
    "               [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4875026  0.42311474 0.4875026 ]]\n",
      "[[0.41276484 0.35848372]]\n",
      "output for input vector 0: [[0.41276484 0.35848372]] \n",
      "[[0.41095957 0.47751518 0.4329071 ]]\n",
      "[[0.40716987 0.35255036]]\n",
      "output for input vector 1: [[0.40716987 0.35255036]] \n",
      "[[0.60587367 0.32739298 0.42555748]]\n",
      "[[0.42857989 0.37145576]]\n",
      "output for input vector 2: [[0.42857989 0.37145576]] \n",
      "[[0.52996405 0.37754067 0.37285223]]\n",
      "[[0.42313121 0.36588853]]\n",
      "output for input vector 3: [[0.42313121 0.36588853]] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "network = NeuralNetwork(2, 3, 2)\n",
    "network.train(input,1)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
